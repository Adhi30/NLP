{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Bag Of Words**"
      ],
      "metadata": {
        "id": "52pvqo5QBf4m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2qPo_tx_PPZ"
      },
      "outputs": [],
      "source": [
        "sentences = [\n",
        "    \"This a fantastic movie of three prisoners who become famous. One of the actors is george clooney and...\",\n",
        "    \"Kind of drawn in by the erotic scenes, only to realize this was one of the most amateurish and unbel...\",\n",
        "    \"Some films just simply should not be remade. This is one of them. In and of itself it is not a bad f...\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "bw = vectorizer.fit_transform(sentences)"
      ],
      "metadata": {
        "id": "96-d6AtJ_-gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvTxbqTxAWhz",
        "outputId": "4bfe2181-637b-48ba-fafa-5d6e0c70258c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<3x41 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 52 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3x41 = 3 sentences , 41 vocabulary**"
      ],
      "metadata": {
        "id": "aAyEtUbiAbKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awfRMKLPAX9G",
        "outputId": "62a6bdee-07e6-4c77-9cd6-91f23ca208ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'this': 35,\n",
              " 'fantastic': 11,\n",
              " 'movie': 21,\n",
              " 'of': 23,\n",
              " 'three': 36,\n",
              " 'prisoners': 26,\n",
              " 'who': 40,\n",
              " 'become': 5,\n",
              " 'famous': 10,\n",
              " 'one': 24,\n",
              " 'the': 33,\n",
              " 'actors': 0,\n",
              " 'is': 15,\n",
              " 'george': 13,\n",
              " 'clooney': 7,\n",
              " 'and': 2,\n",
              " 'kind': 19,\n",
              " 'drawn': 8,\n",
              " 'in': 14,\n",
              " 'by': 6,\n",
              " 'erotic': 9,\n",
              " 'scenes': 29,\n",
              " 'only': 25,\n",
              " 'to': 37,\n",
              " 'realize': 27,\n",
              " 'was': 39,\n",
              " 'most': 20,\n",
              " 'amateurish': 1,\n",
              " 'unbel': 38,\n",
              " 'some': 32,\n",
              " 'films': 12,\n",
              " 'just': 18,\n",
              " 'simply': 31,\n",
              " 'should': 30,\n",
              " 'not': 22,\n",
              " 'be': 4,\n",
              " 'remade': 28,\n",
              " 'them': 34,\n",
              " 'itself': 17,\n",
              " 'it': 16,\n",
              " 'bad': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**"
      ],
      "metadata": {
        "id": "gBEYh2ehBjOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "token = Tokenizer(num_words = 20000)"
      ],
      "metadata": {
        "id": "OGWwrn4AAmTs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token.fit_on_texts(sentences)\n",
        "sequence = token.texts_to_sequences(sentences)"
      ],
      "metadata": {
        "id": "HQ5y30q1B2fJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiRq4dg-CFVB",
        "outputId": "9eaba035-b025-4ed5-fa55-600c4299f197"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 7, 10, 11, 1, 12, 13, 14, 15, 16, 3, 1, 4, 17, 5, 18, 19, 6],\n",
              " [20, 1, 21, 8, 22, 4, 23, 24, 25, 26, 27, 2, 28, 3, 1, 4, 29, 30, 6, 31],\n",
              " [32,\n",
              "  33,\n",
              "  34,\n",
              "  35,\n",
              "  36,\n",
              "  9,\n",
              "  37,\n",
              "  38,\n",
              "  2,\n",
              "  5,\n",
              "  3,\n",
              "  1,\n",
              "  39,\n",
              "  8,\n",
              "  6,\n",
              "  1,\n",
              "  40,\n",
              "  41,\n",
              "  5,\n",
              "  9,\n",
              "  7,\n",
              "  42,\n",
              "  43]]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DYWsWvRCMfU",
        "outputId": "89c6b19e-2918-4f14-b030-26da4c2c54e0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'of': 1,\n",
              " 'this': 2,\n",
              " 'one': 3,\n",
              " 'the': 4,\n",
              " 'is': 5,\n",
              " 'and': 6,\n",
              " 'a': 7,\n",
              " 'in': 8,\n",
              " 'not': 9,\n",
              " 'fantastic': 10,\n",
              " 'movie': 11,\n",
              " 'three': 12,\n",
              " 'prisoners': 13,\n",
              " 'who': 14,\n",
              " 'become': 15,\n",
              " 'famous': 16,\n",
              " 'actors': 17,\n",
              " 'george': 18,\n",
              " 'clooney': 19,\n",
              " 'kind': 20,\n",
              " 'drawn': 21,\n",
              " 'by': 22,\n",
              " 'erotic': 23,\n",
              " 'scenes': 24,\n",
              " 'only': 25,\n",
              " 'to': 26,\n",
              " 'realize': 27,\n",
              " 'was': 28,\n",
              " 'most': 29,\n",
              " 'amateurish': 30,\n",
              " 'unbel': 31,\n",
              " 'some': 32,\n",
              " 'films': 33,\n",
              " 'just': 34,\n",
              " 'simply': 35,\n",
              " 'should': 36,\n",
              " 'be': 37,\n",
              " 'remade': 38,\n",
              " 'them': 39,\n",
              " 'itself': 40,\n",
              " 'it': 41,\n",
              " 'bad': 42,\n",
              " 'f': 43}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stop Words Removal**"
      ],
      "metadata": {
        "id": "87cX1FLfEdkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcRzU-YpEzFB",
        "outputId": "f82c5e2f-900f-4f3e-c6ae-539c4a600086"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "s = set(stopwords.words(\"english\"))\n",
        "s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvsUzBcHCWB8",
        "outputId": "be362847-6767-4ca9-e03e-d13e7ffc275c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " 'if',\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sen in sentences:\n",
        "  print([word for word in sen.split() if word not in s])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12MU4jX2FLL6",
        "outputId": "5a64feac-acae-495e-f77b-54c16db5edaf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'fantastic', 'movie', 'three', 'prisoners', 'become', 'famous.', 'One', 'actors', 'george', 'clooney', 'and...']\n",
            "['Kind', 'drawn', 'erotic', 'scenes,', 'realize', 'one', 'amateurish', 'unbel...']\n",
            "['Some', 'films', 'simply', 'remade.', 'This', 'one', 'them.', 'In', 'bad', 'f...']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "punc = set(string.punctuation)\n",
        "punc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vU6P_YUDGOCZ",
        "outputId": "2f0a5eb8-fc56-4ca5-d377-c69fa5b1db39"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'!',\n",
              " '\"',\n",
              " '#',\n",
              " '$',\n",
              " '%',\n",
              " '&',\n",
              " \"'\",\n",
              " '(',\n",
              " ')',\n",
              " '*',\n",
              " '+',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '/',\n",
              " ':',\n",
              " ';',\n",
              " '<',\n",
              " '=',\n",
              " '>',\n",
              " '?',\n",
              " '@',\n",
              " '[',\n",
              " '\\\\',\n",
              " ']',\n",
              " '^',\n",
              " '_',\n",
              " '`',\n",
              " '{',\n",
              " '|',\n",
              " '}',\n",
              " '~'}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming**"
      ],
      "metadata": {
        "id": "1AZvtMynI-v9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "stemmer.stem(\"News\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "JlPpxIl3F5mo",
        "outputId": "7566b046-25e3-4c17-e3d8-812b26df5d46"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'new'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization**"
      ],
      "metadata": {
        "id": "_mTrfVbcJURd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DO4Sgz9eJrHF",
        "outputId": "c33f3743-26e4-4a15-d21a-d4295d8e1000"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatizer.lemmatize(\"went\",\"v\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "gFjZtUdIJJze",
        "outputId": "f349e685-2107-4fcc-bfa7-5304714ae5ef"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'go'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w2SivAThJzwa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}